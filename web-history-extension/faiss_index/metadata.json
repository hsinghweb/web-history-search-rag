[{"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "Session 1: Introduction to AI, Neural Networks, and Development Tools Skip To Content Dashboard Account Dashboard Courses Calendar Inbox History 3 unread release notes. 3 Help ERA V3 Assignments Session 1: Introduction to AI, Neural Networks, and Development Tools Session 1: Introduction to AI, Neural Networks, and Development Tools Due Oct 19, 2024 by 11:59pm Points None Available after Oct 12, 2024 at 11am Session 1: Introduction to AI, Neural Networks, and Development Tools - Course Structure and Expectations: Overview of the course syllabus, objectives, and assessment methods. - Development Environment Setup: Installing Python, PyTorch, and essential libraries. - Introduction to Modern Coding AI Tools: Overview of tools like Cursor to enhance coding efficiency. - Introduction to Frontend and Backend Concepts: Brief discussion to prepare for integrating AI models into applications. - Fundamentals of AI and Neural Networks: Introduction to core AI concepts, types of neural networks, and their applications. Course Structure and Expectations Introduction: You can find the Course Structure here. We have 30 sessions covering AI fundamentals to advanced generative models, with FrontEnd and BackEnd operations integrated. Learning Objectives: Understand core AI concepts and neural network architectures Gain practical skills in implementing AI models Explore cutting-edge topics in generative AI and large language models. Course Philosophy: Hands-on approach: learn by doing Focus on both theoretical foundations and practical applications Staying current with the rapidly evolving AI landscape What to Expect: Weekly sessions combining lectures, demonstrations, and hands-on exercises Gradual progression from basics to advanced topics Culmination in a capstone project showcasing acquired skills Setting", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_0", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "theoretical foundations and practical applications Staying current with the rapidly evolving AI landscape What to Expect: Weekly sessions combining lectures, demonstrations, and hands-on exercises Gradual progression from basics to advanced topics Culmination in a capstone project showcasing acquired skills Setting Expectations: The learning curve can be steep, but persistence pays off Importance of practice and experimentation outside of class Collaborative learning: engage with peers and instructors Development Environment Setup Introduction: You need to have a laptop (with admin access) to work on this course. You can survive only on Google Colab, but would get frustrating as you go deeper into the course due to the different kinds of files and persistent data that we need to work with. We recommend you to use Visual Studio or Cursor as your developmental IDE but you're free to use the one that you like. Please make sure that you know what different Python Environments are, and ensure that you have the latest Python, PyTorch, and other essential libraries (which will be introduced as the course progresses) You need to create a Google Colab account, which you'll use to train most of the earlier neural networks You also need to create an AWS account, which we will use later to train deeper neural networks. Don't worry, you won't be spending much there. You also need to create HuggingFace account, which you'll use to host your applications (on HuggingFace Space) Introduction to Modern Coding AI tools Introduction: Let's start with a hands-on example! Efficiency and Speed: Tools like Cursor.ai and Claude.dev", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_1", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "much there. You also need to create HuggingFace account, which you'll use to host your applications (on HuggingFace Space) Introduction to Modern Coding AI tools Introduction: Let's start with a hands-on example! Efficiency and Speed: Tools like Cursor.ai and Claude.dev can generate, debug, and optimize code quickly, saving developers time. rather too quickly, hence we'd need to be really good at Git! Enhanced Collaboration: These tools act as virtual coding partners, providing suggestions, improving readability, and refining logic in real-time Adaptability Across Languages: They support multiple programming languages, helping developers work on various projects seamlessly. making this course possible! Contextual Understanding: These AI tools offer deep code context analysis, enhancing the accuracy and relevance of the code they produce Error Reduction: Automated debugging and testing features reduce human errors and improve overall code quality Accessibility for All Skill Levels: Both beginners and experts can leverage AI assistance to boost their productivity and code fluency. Course Structure and Expectations In our Advanced AI course, we will dive into creating complete applications that integrate both frontend and backend technologies. This section aims to give you a solid understanding of these two core concepts and how we will utilize modern frameworks to build AI-driven applications. Frontend: The User Interface (UI) The front end of an application refers to everything the user interacts with directly\u2014essentially, the visual elements of the app. It includes the layout, buttons, menus, input fields, and any other elements that users can see and manipulate. In this course, we will explore: HTML, CSS, JavaScript: The foundational", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_2", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "the user interacts with directly\u2014essentially, the visual elements of the app. It includes the layout, buttons, menus, input fields, and any other elements that users can see and manipulate. In this course, we will explore: HTML, CSS, JavaScript: The foundational web technologies for structuring (HTML), styling (CSS), and adding interactivity (JavaScript) to the user interface. React.js: A powerful frontend library for building dynamic and responsive web applications. It allows developers to create reusable components, which will make building complex UIs more manageable. As we progress in the course, we will use React to design our user interfaces, particularly for AI models where users may need to input data, view predictions, or interact with machine learning models. We will start with a simple introduction to React and its component-based architecture, progressively building more complex frontends that can interface with backend systems and AI models. Backend: The Logic and Data Handling The backend refers to the server side of an application where the core logic, databases, and APIs reside. It handles data storage, processing, and communication between the front end and the server. Key backend concepts and frameworks we will explore include: Python: For the backend, we\u2019ll primarily use Python because of its popularity in AI and machine learning. Python\u2019s ecosystem has a wide variety of frameworks like Flask and FastAPI that make building backend services simple yet powerful. Flask: A lightweight web framework used to build simple and scalable backends. It\u2019s perfect for exposing AI models as APIs that the frontend can consume. FastAPI: Similar to Flask", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_3", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "Flask and FastAPI that make building backend services simple yet powerful. Flask: A lightweight web framework used to build simple and scalable backends. It\u2019s perfect for exposing AI models as APIs that the frontend can consume. FastAPI: Similar to Flask but optimized for high performance and async operations, which will be useful when we scale our applications. Gradio: A library that allows us to quickly build UIs to interface with machine learning models. In this course, we\u2019ll use Gradio to create simple, interactive frontends to showcase AI models' outputs before integrating them into more complex React-based frontends. APIs: Application Programming Interfaces (APIs) will be a core part of our backend systems. You\u2019ll learn how to expose AI models as APIs that can be consumed by frontend clients or other services. REST APIs (using Flask or FastAPI) will be our main focus. Integrating Frontend and Backend As we progress, we will move from isolated frontend or backend projects to fully integrated applications. Here\u2019s how the integration will unfold: Gradio for Early Prototypes: Initially, you\u2019ll use Gradio to quickly test and deploy AI models with simple UIs. This will help you understand the complete model lifecycle, from training to deployment. Backend APIs (Python): Once we understand how to deploy models with Gradio, we\u2019ll learn how to create more flexible and scalable backend services using Flask or FastAPI. You will expose your models as REST APIs that can be called from the frontend. React-based Frontends: After setting up the backend, we\u2019ll start building more complex frontends with React.js, focusing", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_4", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "flexible and scalable backend services using Flask or FastAPI. You will expose your models as REST APIs that can be called from the frontend. React-based Frontends: After setting up the backend, we\u2019ll start building more complex frontends with React.js, focusing on creating responsive UIs that can interact with the backend APIs you\u2019ve built. This will enable users to interact with AI models in real-time. Hosting and Deployment: Lastly, we will cover how to host and deploy these applications. This involves using cloud platforms like AWS or Heroku for deploying both the frontend and backend. We\u2019ll also explore Docker for containerization, ensuring your application can be easily scaled and deployed across environments. Frameworks and Tools in Focus Frontend: React.js, HTML, CSS, JavaScript Backend: Python, Flask, FastAPI, Gradio, REST APIs Deployment: Docker, AWS, Heroku Fundamentals of AI and Neural Network or Breaking it down! Receptive Fields or Attention Spans: The main logic behind getting AI where it is today was to solve the core problem of receptive fields or attention spans. Let's look at these 2 modalities below: do it. How can you now believe her? I hope you'll not be able to identify what this image is all about or what this text is referring to!! You need more context in both cases. In the case of the images, it is called the \"Receptive Field\" and in the case of text it is called the \"Attention Span\". Let's look at the complete information. We all saw her kill that man, we had surrounding CCTV footage that captured", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_5", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "the images, it is called the \"Receptive Field\" and in the case of text it is called the \"Attention Span\". Let's look at the complete information. We all saw her kill that man, we had surrounding CCTV footage that captured it. We even found traces of blood on her clothes. But she said she didn't do it. How can you now believe her? The moment you see the complete picture you can make a lot more inferences! But we\u2019re getting of ourselves here! I\u2019m supposed to cover this in sections 4-6. Today we\u2019ll spend our time in understanding two things. Not only we need to have a \u201ccomplete\u201d picture, we should also be able to break things down to understand the building blocks. For text, it\u2019s simple, the words. But for images, I have to convince you a lot more. You see since our language is made of words and we need to use a lot of them to process what\u2019s being said, we have this intuitive understanding that \u201cwords\u201d are the building blocks for NLP. For images it\u2019s not so obvious. It\u2019s not ONLY the colors that are required to \u201csee\u201d something. It is actually small edges and gradients in all possible colors. These edges and gradients are combined to make textures and patterns. These textures and patterns are combined to make part of objects. And these parts of objects are then combined to make objects! These objects are then combined to make Scenes. This is very similar to how characters are used to make", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_6", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "textures and patterns are combined to make part of objects. And these parts of objects are then combined to make objects! These objects are then combined to make Scenes. This is very similar to how characters are used to make words. Words are then used to make sentences (actually only when words are in a sentence do they have the right meaning). Then sentences are combined to make a paragraph. Paragraphs are then converted into whole stories or contexts! Today our focus is to be able to break things down. We need to break a song into building elements and so on. Building the Intuition: Data Representation An ultra-dense connected network of flowing information inwards! What do we observe in the image above? In this course, we will focus on vision and text (with a bit of audio as well). Let us see how vision is represented in human brains first. Human Eye Human Eye Made on MidJourney! The human eye is the result of billions of years of evolution (read this amazing post on the evolution of eyes) Links to an external site. , and what our eyes and brain do is just magical! Those beautiful orange structures are actually muscles that pull the lens to help us focus, exactly like, how you'd use a DSLR Camera lens. Light falls on the photo sensors in the center (just as it would on a DSLR/phone) Source Links to an external site. Similar to our eyes, humans have invented the whole sensing system and this is what", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_7", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "DSLR Camera lens. Light falls on the photo sensors in the center (just as it would on a DSLR/phone) Source Links to an external site. Similar to our eyes, humans have invented the whole sensing system and this is what it looks like: A Camera lens and sensor system If we use an electron microscope and look at our retina, this is what we'll get: Rods and Cones under electron microscope Rods: Cones ratio is 20:1. Rods see BnW and Cones see colors. This is what a camera sensor looks like: Camera CMOS sensor under a microscope And just like a camera, the human eye also has three types Links to an external site. of cones: red, green, and blue! They just need a lot of light to work, and hence so many rods. Our optic nerve is connected to our visual cortex, where the processing takes place: The visual path from the retina to the visual cortex One thing that we should know is, that saying that our cones see red, green, and blue, is misleading. They are sensitive to these colors, and many around them, as can be seen in this image: Source Links to an external site. Understanding this concept is super critical for us! Channels or Breaking it down! Any color can be made through a combination of different kinds of \"primary color\" combinations. RGB is not the only model. We can use CMYK, CMY, YIQ, LAB, HSV, HSL, HVC, Munsell, YUV, YCbCr, HSI, CIE, XYZ, etc! Representing same image in different", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_8", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "be made through a combination of different kinds of \"primary color\" combinations. RGB is not the only model. We can use CMYK, CMY, YIQ, LAB, HSV, HSL, HVC, Munsell, YUV, YCbCr, HSI, CIE, XYZ, etc! Representing same image in different color models. What I want you to focus on, is the fact that there are numerous ways of representing the same complex data. RGB creates different colors And you must realize this. On a side note: I want you to realize the fact that nearly the same word sound can be made in 293 different writing systems! And the same idea can be represented in any of the 19500 languages and dialects, just in India Links to an external site. ! Again the emphasis here is to understand the fact, that a \"complex\" concept could be represented in many many forms. Just like different units of measurement: For distance Kilometer (km), Meter (m), Centimeter (cm), Millimeter (mm), Mile (mi), Yard (yd), Foot (ft), Inch (in), Nautical mile (nm), Astronomical Unit (AU), Light-year (ly) For Weight Kilogram (kg), Gram (g), Milligram (mg), Microgram (\u00b5g), Pound (lb), Ounce (oz), Carat (ct), Ton (t), Metric tonne (t), Atomic mass unit (amu), Moles (M) Expressing the same concept in these different units does not change the underlying value, though the context in which we might use them is different. Now I want you to guess what is this Source Links to an external site. In 3D graphics, we actually never see curved edges. Ultimately it is broken down in a", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_9", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "context in which we might use them is different. Now I want you to guess what is this Source Links to an external site. In 3D graphics, we actually never see curved edges. Ultimately it is broken down in a quantized line! Everything in graphics in creating using a line. We have to use a lot of tricks to fake smoothness LeftL: Original image. Right: Antialiased image So we have seen a few simpler \"primary\" representations (like colors, edges, and characters) for complex things (like images, and concepts). And there is an ocean of different kinds of representations between these primary and complex representations. For instance Edges & Gradients, Textures, and Patterns, Part of Objects and Objects Characters, Words, Sentences, Paragraphs, Book Air vibration, Phoneme, Sound of a word, Sound of a full sentence Etc. When I segregated these concepts above in a bucket like \"Sound of words\" or \"Textures and Patterns\" or \"Sentences\", I was creating something that we know as a \"Block\" of layers. Every layer would have 100s of thousands of channels, where each channel is responsible only for one thing. For instance: At some higher-level blocks, a few of the channels could be vocals, guitar, bass , synth, and drums. When we think in terms of channels, we can \"split\" the song being played above into its individual components, like the sound made by piano, guitar, bass, drums, etc. When everything comes together we get magic! Let's listen to this: This was generated by MusicLM Links to an external site. using this", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_10", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "being played above into its individual components, like the sound made by piano, guitar, bass, drums, etc. When everything comes together we get magic! Let's listen to this: This was generated by MusicLM Links to an external site. using this prompt: A fusion of reggaeton and electronic dance music, with a spacey, otherworldly sound. Induces the experience of being lost in space, and the music would be designed to evoke a sense of wonder and awe, while being danceable. In the image below, one of the \"lowest-level blocks\" could be the alphabet. What is the maximum number of channels possible in this block (assuming the size of the characters does not matter)? Typography Art In the image below, a somewhat mid-level block could be represented by the ingredients: Ingredients for Rice Pulao Based on the concept, these channels (individual concepts/features) would be anything. Like material type in this image: Car segmented based on its material. and component level here: All the components of the car In both cases, we're using different kinds of channels, but I want you to be convinced that in the former image, channels were more \"basic\" than in the latter image. The later image had more \"complex\" channels. What did we learn till now? Anything complex can be made from simpler stuff! Well, that is what AI is all about. How do we break down the problem into the simplest possible stuff, that can then be combined in trillions of ways to represent anything that we may want? Origin or The Story", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_11", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "Well, that is what AI is all about. How do we break down the problem into the simplest possible stuff, that can then be combined in trillions of ways to represent anything that we may want? Origin or The Story of Two Missing Cats CAT 1 Image created on MidJourney. CAT 2 Image created on MidJourney. CAT 1: Experimental Results If you are interested in learning more, read more about retinotopic maps and fMRI When we see the image on the left, it gets \"printed\" on our brains, literally, as shown in the image on the right! CAT 2: Experimental Results Would highly recommend you to look at this video and subscribe to this channel! In our own visual cortex, we start with simple shapes (edges and gradients), combine them to make complex shapes (patterns/textures/part of objects), and finally get to see (the objects or the scene) If you are interested in the above topic, please read more on this link Links to an external site. . Something dramatically similar happens in the deep neural networks! And being artificial, we can actually get them to show us what happens inside them! A Kernel is a feature extractor, and these feature extractors are what is learned in a DNN. Above you see an \"extrapolation\" of the feature a kernel extracts as if the whole image was just that feature. We can even query the very first \"block\" of the DNN and see individual features of each \"kernel\" extract: Above what you see is what a kernel or", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_12", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "a kernel extracts as if the whole image was just that feature. We can even query the very first \"block\" of the DNN and see individual features of each \"kernel\" extract: Above what you see is what a kernel or feature extractor or learned matrix exacts. Since this kernel was 11x11 in size it can extract only 11x11 features. If we extrapolate this 11x11 onto a big image, we'll see something similar to the image above. Let's look at this time-lapse to appreciate how simple strokes can make something really beautiful. Here we spend a few minutes, while our brain-eye system will spend less than 100ms Again, the concept we are stying to learn here is that complex structures or things can be built from simple strokes, similar to the fact that we are communicating right now using just 26 alphabets! Core Concepts We need to understand a few other concepts before we jump to Convolutions. One such concept can be understood from the difference between a Rolling Shutter Camera and a Global/Total Shutter Camera The reason behind these effects is how the pixels are actually created. Rolling vs Global Shutter Of course, Global Shutter cameras always make sense, but they are complex, expensive, and bulky to manufacture. All mobile phones use Rolling Shutter cameras, but now the speed of capture has increased a lot, hence we don't see these effects. Rolling Shutters are digital in nature, and Global Shutters are mechanical. DNNs would use a rolling shutter kind of concept, but our brains use global", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_13", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "now the speed of capture has increased a lot, hence we don't see these effects. Rolling Shutters are digital in nature, and Global Shutters are mechanical. DNNs would use a rolling shutter kind of concept, but our brains use global shutters kind of capture. A slightly extended concept was used during the Jurrasic era by computer vision engineers. Its called Sliding Windows Sliding window To actually implement a global shutter kind of processing in DNNs, we would need to change the hardware itself. Today hardware processing is based on \"clock ticks\" and we can process a small amount of data in a very very small tick In both of these examples, let us focus on the \"area in context\". In the images above we either have a horizontal block or a square block of data that we are processing or \"reading\". Now \"reading\" here basically means, that we are storing it for some kind of processing. The pixels that are going to be stored will be worked upon by some algorithm, which ultimately would mean that some numbers would be multiplied/divided/added/etc to them. These numbers are called kernels. Convolutions A 3x3 kernel on a 4x4 image. Here we are \"reading\" 3x3 numbers on a 4x4 image. The moment we read these 3x3 pixels we multiply them by some 3x3 other numbers (identified by a DNN). These \"other 3x3 numbers\" are called Kernels. Let's check this quickly Links to an external site. So, there exist simple 3x3 matrix numbers that can easily identify basic lines/edges. A 3x3", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_14", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "by some 3x3 other numbers (identified by a DNN). These \"other 3x3 numbers\" are called Kernels. Let's check this quickly Links to an external site. So, there exist simple 3x3 matrix numbers that can easily identify basic lines/edges. A 3x3 kernel on a 5x5 image. So if we convolve a 3x3 kernel on a 5x5 image, the output we would create will have a resolution of 3x3. This is true only when: we are not going outside of the image (by adding imaginary numbers). This by the way is called padding. We can add imaginary black/white (0/1/255) numbers and then allow our kernel to slide out of the image (why would we want to do this>>Session 3) we are not using a stride of more than 1, i.e. we will cover each 3x3 section immediately after 1 pixel In the images above, our kernel skips/jumps only 1 pixel. If we were to jump/skip 2 pixels, then our kernel has a stride of 2 (pixels). Fully Connected Layers Convolutions are fairly new compared to Fully Connected Layers, and quickly took over, the moment we had enough computing because FC layers suffered from a serious issue (which transformers solved!) Let's look at this image: Digit 3: represented by 5x8 pixels We can unroll this image and make something like this: Same Digit 3: represented by 40x1 pixels. More about these in the next session. Receptive Fields and Attention The most important image in ERA for Vision: Receptive Fields The most important image in ERA for NLP/Audio: Let's check", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_15", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "like this: Same Digit 3: represented by 40x1 pixels. More about these in the next session. Receptive Fields and Attention The most important image in ERA for Vision: Receptive Fields The most important image in ERA for NLP/Audio: Let's check the source Links to an external site. for animation. Important Note on Vision vs NLP. In the case of the language, we have broken down concepts into words. Each word means something, and based on the context (bank example) we know what it means. In fact, we (sort of) have a fixed exhaustive dictionary of words that we can download and make GPT4 out of! In the world of 3D graphics, we have done the same thing. We have these edges, that make polygons. Using these polygons we make primitives like spheres, cubes, etc., and complex shapes. But we are (not all) graphics designers and we don't have the intuitions to break down the big picture into smaller pieces. But for a moment let's imagine that we indeed had characters and words like concepts in vision. The next step would have been to directly use them to define the image/world (just like in NLP). And now you immediately start seeing where exactly the architectures for Vision and NLP would meet! The convolutions we saw above would help us create these vision words (edges, gradients, patterns), and then Fully Connected Layers (Transformers) would help us connect them with language in a single model! The Assignment Your quiz questions are on a separate quiz listed as \"S1 Quiz\".", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_16", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "help us create these vision words (edges, gradients, patterns), and then Fully Connected Layers (Transformers) would help us connect them with language in a single model! The Assignment Your quiz questions are on a separate quiz listed as \"S1 Quiz\". Make Something! We saw 2 examples of using Cussor to make Chrome Extensions! Be creative, make something! Here are some examples (you're not supposed to use these, these examples are to give you some ideas): Find and list the latest movies on NetFlix Find the next Cricket Match or F1 GrandPrix Change the font on any website to anything that you like! Make a clipboard that stores all of your \"copy\" texts on your browser A small web browser-based game! Do not spend more than 1 hour doing this, this may be too addictive! Follow the steps. It is better to start from scratch, rather than to fix big. Next time improve your prompt! Once done, upload a short video of it working on YouTube and share the link (to capture the video, you can use \"Snip & Sketch\", OBS, etc) zip your folder and share the code. Did you share it on LinkedIn? If yes, please share the link and get 200 pts extra! This is optional! 200 + 200 Pts 0.99 Weeks Videos Studio Google Meet Transcripts Download Transcripts Previous Next", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_17", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/courses/10442811/assignments/50225496?module_item_id=118231724", "title": "Session 1: Introduction to AI, Neural Networks, and Development Tools", "chunk": "Google Meet Transcripts Download Transcripts Previous Next", "chunk_id": "https___canvas.instructure.com_courses_10442811_assignments_50225496?module_item_id=118231724_18", "timestamp": "2025-04-27T03:22:36.569Z"}, {"url": "https://canvas.instructure.com/", "title": "Dashboard", "chunk": "Dashboard Skip To Content Dashboard Account Dashboard Courses Calendar Inbox History 3 unread release notes. 3 Help Dashboard Dashboard Dashboard Options Course image for EAG V1 EAG V1 EAG V1 Choose a color or course nickname or move course card for EAG V1 Assignments - EAG V1 Discussions - EAG V1 Course image for ERA V3 ERA V3 ERA V3 Choose a color or course nickname or move course card for ERA V3 Assignments - ERA V3 Discussions - ERA V3 To Do Recent Feedback Session 5 - Assignment QnA EAG V1 2,000 out of 1,000 \"Excellent work! :)\" Session 21 - Assignment QnA 2,000 out of 2,000 \"https://github.com/hsinghweb/era-v3-s21-t3dhttps://www.youtube.com/watch?v=clsc0eiyu38\" Session 6 - Assignment QnA EAG V1 5 more in the past two weeks \u2026 Start a New Course View Grades By Instructure Privacy Policy Cookie Notice Acceptable Use Policy Facebook X.com", "chunk_id": "https___canvas.instructure.com__0", "timestamp": "2025-04-27T03:23:33.595Z"}, {"url": "https://canvas.instructure.com/courses/11514125", "title": "EAG V1", "chunk": "EAG V1 Skip To Content Dashboard Account Dashboard Courses Calendar Inbox History 3 unread release notes. 3 Help EAG V1Modules EAG V1 EAG V1 Course Modules Collapse All EAG V1 EAG V1 Page Welcome to EAG V1 Page Syllabus Page Session 1 - Foundations of the Transformer Architecture Quiz Session 1 - Assignment QnA Page Session 2 - Modern Language Model Internals Quiz Session 2 - Assignment QnA Quiz Session 2 - Quiz Page Session 3 - Introduction to Agentic AI Quiz Session 3 - Assignment QnA Page Session 4 - Model Context Protocol (MCP) Quiz Session 4 - Assignment QnA Page Session 5 - Planning and Reasoning with Language Models Quiz Session 5 - Assignment QnA Page Session 6 - Agentic Architecture Quiz Session 6 - Assignment QnA Page Session 7 - Memory Quiz Session 7 - Assignment QnA Page Session 8 - Tool Use, External APIs & RAG 2 Quiz Session 8 - Assignment QnA View Course Stream View Course Calendar View Course Notifications To Do Recent Feedback Session 5 - Assignment QnA EAG V1 2,000 out of 1,000 \"Excellent work! :)\" Session 6 - Assignment QnA EAG V1 Session 4 - Assignment QnA EAG V1 875 out of 500", "chunk_id": "https___canvas.instructure.com_courses_11514125_0", "timestamp": "2025-04-27T03:23:35.097Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "Session 7 - Memory: EAG V1 Skip To Content Dashboard Account Dashboard Courses Calendar Inbox History 3 unread release notes. 3 Help EAG V1PagesSession 7 - Memory View All Pages Session 7 - Memory Session 7 - (RAG) Memory, Context, and State Management in AI Agents - Vector databases, RAG (Retrieval-Augmented Generation), and memory layers. - Short-term vs. long-term memory strategies. - Case study: Using LangChain for context-aware agents. Little Bit of Python And Code First The embedding models We've seen LLMs generate embeddings per token, but there are specialized models (Google Gemini Links to an external site. or anyone else) that can predict SINGLE embedding for whatever is sent to it. This is useful because now we can get SINGLE embeddings for each document, query, image, snippet, etc, and then perform a similarity search on it. embeddings.py Download embeddings.py embeddings_ollama.py Download embeddings_ollama.py Embeddings are numerical representations of meaning You can print, store, or index them for search You chose the task_type: RETREIVAL_QUERY: if you're going to search with it RETERIVAL_DOCUMENT: if you're indexing chunks CLUSTERING: if you're doing clustering embeddings_compare.py Download embeddings_compare.py embeddings_compare_ollama.py Download embeddings_compare_ollama.py Let us say we have these sentences: The output would be something like: Embeddings allow you to compare meaning, not just keywords Gemini or other models understand relationships between science/biology topics Cosine similarity is the code metric behind RAG search, document clustering, memory matching, etc. The FAISS module and chunking Vector Storage: storing and searching over embeddings (i.e. high-dimensional numeric representations of text, images, code, etc.). Think of it as:", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_0", "timestamp": "2025-04-27T03:23:43.818Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "topics Cosine similarity is the code metric behind RAG search, document clustering, memory matching, etc. The FAISS module and chunking Vector Storage: storing and searching over embeddings (i.e. high-dimensional numeric representations of text, images, code, etc.). Think of it as: A memory place where instead of words, you store meaning as numbers\". Each sentence or document is turned into a vector like: Then, when you want to find \"similar\" content, you compare vectors using distance functions like: closing similarity L2 distance FAISS What is FAISS? Facebook AI Similarity Search is a fast, scalable C++/Python library for efficient similarity search and clustering of dense vectors. It is used for: RAG retrieval Semantic Search Recommendation systems LLM memory systems Let's look at some SIMPLE code. You have a bunch of quotes. Someone gives you a new quote, and you want to find a similar one in spirit. faiss helps you do this in milliseconds. faiss_simple.py Download faiss_simple.py faiss_simple_ollama.py Download faiss_simple_ollama.py What's happening? Embedding model (Google Gemini Links to an external site. or anyone else), converts our text into a SINGLE embedding of 3k dimensions. FAISS stores these dimensions and indexes them for fast search and retrieval You pass a new query vector to the Embedding model and get one more 3k embedding FAISS finds the \"nearest neighbor\" - i.e. semantically most similar text. Let's look at some MEDIUM complexity code. Upgrade: let's use FAISS to build a mini search engine over a large set (mine is too small \ud83e\udd72) of short jokes, tweets, or one-liners faiss_medium.py Download faiss_medium.py", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_1", "timestamp": "2025-04-27T03:23:43.818Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "i.e. semantically most similar text. Let's look at some MEDIUM complexity code. Upgrade: let's use FAISS to build a mini search engine over a large set (mine is too small \ud83e\udd72) of short jokes, tweets, or one-liners faiss_medium.py Download faiss_medium.py faiss_medium_ollama.py Download faiss_medium_ollama.py Features: Top-k search (you can get top-3/4/5/k search results) Normalize vectors (for cosine similarity) Metadata association (like category, joke ID) Returns both the best match and a few close contenders Let's go full-on advanced! We're now building a mini-RAG-ready search engine with: Document chunking (with overlap) Embeddings from Gemini FAISS for similarity search Metadata and chunk-to-doc mapping faiss_advanced.py Download faiss_advanced.py (or faiss_advanced_ollama.py Download faiss_advanced_ollama.py ) and documents.zip Download documents.zip The llama-index framework llama-index is a framework for building RAG pipelines using LLMs It answers this question: How do I feed the right context to my LLM so it can give me a grounded, accurate response? Think of it as a bridge between your data and your LLM. Why do we need llama-index ? Working with LLMs involves three messy problems: You have a lot of information (doc, files, memory, transcripts...) LLMs have limited context window, can't see it all You want the LLM to answer grounded on your data, not hallucinate You want the RAG pipeline to be easy to build, modular, and debuggable. llama-index solves all of these: Feature What it does Document loaders Reads PDFs, Notion, folders, URLs, SQL, etc Chunkers Splits data into meaning pieces (with overlap) Embedders Supports OpenAI, Gemini, local models, etc Index Builders Creates VectorIndex , KeywordIndex", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_2", "timestamp": "2025-04-27T03:23:43.818Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "debuggable. llama-index solves all of these: Feature What it does Document loaders Reads PDFs, Notion, folders, URLs, SQL, etc Chunkers Splits data into meaning pieces (with overlap) Embedders Supports OpenAI, Gemini, local models, etc Index Builders Creates VectorIndex , KeywordIndex , SummaryIndex . Retrievers Find top-k most relevant chunks Query Engine Combines context + prompt + LLM for answer Memory Supports chat history, summarization, compression Agent Support Plug it into any agent! We'll refer to it later on, and are already building something like it. Why Memory Matters? Imagine this: You're having a conversation with your AI agent You: \"Hey, can you suggest some beginned-friendly Python Course?\" Agent: \"Sure, You can try Codeacademy or freeCodeCamp.\" You: Nice.. You (10 minutes later): \"Which of these had the NumPy tutorial?\" Agent: \"Sorry, I don't recall our previous conversation.\" Magic Gone! Welcome to stateless AI - where each interaction is like talking to someone with short-term amnesia. A (not-so) honorable mention: Our Current Setup Before we go further, let's talk about the beautiful little abomination we built in Session 6. In talk2mcp.py , we \"stored memory\" using a global list called iteration_response : We proudly recorded entire system memories like we were writing a medieval scroll: No indexes No semantics Just one long log of past tool calls glues into the prompt This isn't scalable! So What Is Memory in Agentic AI? Let's now look at how memory should be implemented in intelligent agents. Memory \u2260 Prompt Length Sticking facts in a long prompt isn't memory. It's just... hoping", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_3", "timestamp": "2025-04-27T03:23:43.818Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "into the prompt This isn't scalable! So What Is Memory in Agentic AI? Let's now look at how memory should be implemented in intelligent agents. Memory \u2260 Prompt Length Sticking facts in a long prompt isn't memory. It's just... hoping the LLM still cares by the time it gets to line 237. Real memory in AI agents should be: Searchable: Can find what's relevant to the current task Structured: Stores what, when and why something happened Composable: Can be used to influence future decisions Persistent: Doesn't disappear when the program restarts. Memory = What Makes Agents Feel Personal Think about Siri or Alexa. They respond to commands. But do they know you? Can they tell what projects you're working on? What tools you've used before? Now imaging an AI agent that remembers: Your favourite color scheme when generating a UI That you prefer command-line tools over web GUIs You once asked it to explain Fourier Tranforms using only pizza metaphors That's not just \"better UX\". That's cognitive continuity. Memory in Layers (Real Agent Style) In Session 6, we built the Perception \u2192 Memory \u2192 Decision \u2192 Action pipeline. Here\u2019s where memory fits in as a first-class citizen. Layer With Memory (Good) Without Memory (Bad) Perception Understands new input in context Reacts blindly to each new utterance Memory Searches prior preferences, facts, and results Has no clue what's happened before Decision-Making Plans based on both current and past inputs Only has the present moment to go on Action Feels intelligent and consistent Feels robotic and repetitive What", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_4", "timestamp": "2025-04-27T03:23:43.818Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "Memory Searches prior preferences, facts, and results Has no clue what's happened before Decision-Making Plans based on both current and past inputs Only has the present moment to go on Action Feels intelligent and consistent Feels robotic and repetitive What We Should Have done? Instead of this (from our existing code): We should've done this: Then, inject that intelligently into the LLM prompt. Short-Term vs Long-Term Memory Short-Term: Lives in prompt or variables Good for holding context within a task Lost between runs, or if it exceeds the token limits Long-Term: Stores externally (e.g., vector DB) Can persist across sessions, days, and users Includes past decisions, tool calls, and user feedback. tldr; Short-Term memory is whatever fits inside the token window. Long-term memory is what you retrieve and then inject into that window. We'll build this in this session, using a lightweight in-memory DB ( faiss ) and a retrieval module we plug into our MCP agent loop. Vector DBs & embeddings Key Concepts: Embeddings: Vectors that represent meaning. Similarity Search: Find chunks most semantically similar to a query FAISS: Facebook AI Similarity Search - fast, memory-efficient index Embeddings are like a meaning ingerprint. You don't need exact words - you need closeness in idea space. Remember faiss_simple.py? \"Birds of a feather\" - matches \"People with similar traits stick together\". Let's look at a RAG implementation using MCP The zipped Download zipped code (with code and my index as well) Assignment Build a Chrome plugin that: For every web page that you visit (skip confidential ones", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_5", "timestamp": "2025-04-27T03:23:43.818Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "similar traits stick together\". Let's look at a RAG implementation using MCP The zipped Download zipped code (with code and my index as well) Assignment Build a Chrome plugin that: For every web page that you visit (skip confidential ones like Gmail, WhatsApp, etc), builds a nomic embedding (or any other model that you can run) and then builds an FAISS index with URL Please remember that you only need the index file, so you can do it on Google Colab and then download this index file as well When you search within your plugin, it opens the website where that content is and highlights it as well! 1000 Pts for above OR 2000 Pts total if you come up with an amazing idea to use RAG (locally or on browser). Share your YouTube Video and GitHub link. HOPING YOU REMEMBER TO ALWAYS HAVE THOSE 5 FILES (AGENT, PERCEPTION, MEMORY, ACTION AND DECISION) Video Studio (Transcript Download Transcript ) GMeet Previous Next", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_6", "timestamp": "2025-04-27T03:23:43.818Z"}]