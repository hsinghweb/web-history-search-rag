[{"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "Session 7 - Memory: EAG V1 Skip To Content Dashboard Account Dashboard Courses Calendar Inbox History 3 unread release notes. 3 Help EAG V1PagesSession 7 - Memory View All Pages Session 7 - Memory Session 7 - (RAG) Memory, Context, and State Management in AI Agents - Vector databases, RAG (Retrieval-Augmented Generation), and memory layers. - Short-term vs. long-term memory strategies. - Case study: Using LangChain for context-aware agents. Little Bit of Python And Code First The embedding models We've seen LLMs generate embeddings per token, but there are specialized models (Google Gemini Links to an external site. or anyone else) that can predict SINGLE embedding for whatever is sent to it. This is useful because now we can get SINGLE embeddings for each document, query, image, snippet, etc, and then perform a similarity search on it. embeddings.py Download embeddings.py embeddings_ollama.py Download embeddings_ollama.py Embeddings are numerical representations of meaning You can print, store, or index them for search You chose the task_type: RETREIVAL_QUERY: if you're going to search with it RETERIVAL_DOCUMENT: if you're indexing chunks CLUSTERING: if you're doing clustering embeddings_compare.py Download embeddings_compare.py embeddings_compare_ollama.py Download embeddings_compare_ollama.py Let us say we have these sentences: The output would be something like: Embeddings allow you to compare meaning, not just keywords Gemini or other models understand relationships between science/biology topics Cosine similarity is the code metric behind RAG search, document clustering, memory matching, etc. The FAISS module and chunking Vector Storage: storing and searching over embeddings (i.e. high-dimensional numeric representations of text, images, code, etc.). Think of it as:", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_0", "timestamp": "2025-04-27T03:27:43.710Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "topics Cosine similarity is the code metric behind RAG search, document clustering, memory matching, etc. The FAISS module and chunking Vector Storage: storing and searching over embeddings (i.e. high-dimensional numeric representations of text, images, code, etc.). Think of it as: A memory place where instead of words, you store meaning as numbers\". Each sentence or document is turned into a vector like: Then, when you want to find \"similar\" content, you compare vectors using distance functions like: closing similarity L2 distance FAISS What is FAISS? Facebook AI Similarity Search is a fast, scalable C++/Python library for efficient similarity search and clustering of dense vectors. It is used for: RAG retrieval Semantic Search Recommendation systems LLM memory systems Let's look at some SIMPLE code. You have a bunch of quotes. Someone gives you a new quote, and you want to find a similar one in spirit. faiss helps you do this in milliseconds. faiss_simple.py Download faiss_simple.py faiss_simple_ollama.py Download faiss_simple_ollama.py What's happening? Embedding model (Google Gemini Links to an external site. or anyone else), converts our text into a SINGLE embedding of 3k dimensions. FAISS stores these dimensions and indexes them for fast search and retrieval You pass a new query vector to the Embedding model and get one more 3k embedding FAISS finds the \"nearest neighbor\" - i.e. semantically most similar text. Let's look at some MEDIUM complexity code. Upgrade: let's use FAISS to build a mini search engine over a large set (mine is too small \ud83e\udd72) of short jokes, tweets, or one-liners faiss_medium.py Download faiss_medium.py", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_1", "timestamp": "2025-04-27T03:27:43.710Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "i.e. semantically most similar text. Let's look at some MEDIUM complexity code. Upgrade: let's use FAISS to build a mini search engine over a large set (mine is too small \ud83e\udd72) of short jokes, tweets, or one-liners faiss_medium.py Download faiss_medium.py faiss_medium_ollama.py Download faiss_medium_ollama.py Features: Top-k search (you can get top-3/4/5/k search results) Normalize vectors (for cosine similarity) Metadata association (like category, joke ID) Returns both the best match and a few close contenders Let's go full-on advanced! We're now building a mini-RAG-ready search engine with: Document chunking (with overlap) Embeddings from Gemini FAISS for similarity search Metadata and chunk-to-doc mapping faiss_advanced.py Download faiss_advanced.py (or faiss_advanced_ollama.py Download faiss_advanced_ollama.py ) and documents.zip Download documents.zip The llama-index framework llama-index is a framework for building RAG pipelines using LLMs It answers this question: How do I feed the right context to my LLM so it can give me a grounded, accurate response? Think of it as a bridge between your data and your LLM. Why do we need llama-index ? Working with LLMs involves three messy problems: You have a lot of information (doc, files, memory, transcripts...) LLMs have limited context window, can't see it all You want the LLM to answer grounded on your data, not hallucinate You want the RAG pipeline to be easy to build, modular, and debuggable. llama-index solves all of these: Feature What it does Document loaders Reads PDFs, Notion, folders, URLs, SQL, etc Chunkers Splits data into meaning pieces (with overlap) Embedders Supports OpenAI, Gemini, local models, etc Index Builders Creates VectorIndex , KeywordIndex", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_2", "timestamp": "2025-04-27T03:27:43.710Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "debuggable. llama-index solves all of these: Feature What it does Document loaders Reads PDFs, Notion, folders, URLs, SQL, etc Chunkers Splits data into meaning pieces (with overlap) Embedders Supports OpenAI, Gemini, local models, etc Index Builders Creates VectorIndex , KeywordIndex , SummaryIndex . Retrievers Find top-k most relevant chunks Query Engine Combines context + prompt + LLM for answer Memory Supports chat history, summarization, compression Agent Support Plug it into any agent! We'll refer to it later on, and are already building something like it. Why Memory Matters? Imagine this: You're having a conversation with your AI agent You: \"Hey, can you suggest some beginned-friendly Python Course?\" Agent: \"Sure, You can try Codeacademy or freeCodeCamp.\" You: Nice.. You (10 minutes later): \"Which of these had the NumPy tutorial?\" Agent: \"Sorry, I don't recall our previous conversation.\" Magic Gone! Welcome to stateless AI - where each interaction is like talking to someone with short-term amnesia. A (not-so) honorable mention: Our Current Setup Before we go further, let's talk about the beautiful little abomination we built in Session 6. In talk2mcp.py , we \"stored memory\" using a global list called iteration_response : We proudly recorded entire system memories like we were writing a medieval scroll: No indexes No semantics Just one long log of past tool calls glues into the prompt This isn't scalable! So What Is Memory in Agentic AI? Let's now look at how memory should be implemented in intelligent agents. Memory \u2260 Prompt Length Sticking facts in a long prompt isn't memory. It's just... hoping", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_3", "timestamp": "2025-04-27T03:27:43.710Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "into the prompt This isn't scalable! So What Is Memory in Agentic AI? Let's now look at how memory should be implemented in intelligent agents. Memory \u2260 Prompt Length Sticking facts in a long prompt isn't memory. It's just... hoping the LLM still cares by the time it gets to line 237. Real memory in AI agents should be: Searchable: Can find what's relevant to the current task Structured: Stores what, when and why something happened Composable: Can be used to influence future decisions Persistent: Doesn't disappear when the program restarts. Memory = What Makes Agents Feel Personal Think about Siri or Alexa. They respond to commands. But do they know you? Can they tell what projects you're working on? What tools you've used before? Now imaging an AI agent that remembers: Your favourite color scheme when generating a UI That you prefer command-line tools over web GUIs You once asked it to explain Fourier Tranforms using only pizza metaphors That's not just \"better UX\". That's cognitive continuity. Memory in Layers (Real Agent Style) In Session 6, we built the Perception \u2192 Memory \u2192 Decision \u2192 Action pipeline. Here\u2019s where memory fits in as a first-class citizen. Layer With Memory (Good) Without Memory (Bad) Perception Understands new input in context Reacts blindly to each new utterance Memory Searches prior preferences, facts, and results Has no clue what's happened before Decision-Making Plans based on both current and past inputs Only has the present moment to go on Action Feels intelligent and consistent Feels robotic and repetitive What", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_4", "timestamp": "2025-04-27T03:27:43.710Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "Memory Searches prior preferences, facts, and results Has no clue what's happened before Decision-Making Plans based on both current and past inputs Only has the present moment to go on Action Feels intelligent and consistent Feels robotic and repetitive What We Should Have done? Instead of this (from our existing code): We should've done this: Then, inject that intelligently into the LLM prompt. Short-Term vs Long-Term Memory Short-Term: Lives in prompt or variables Good for holding context within a task Lost between runs, or if it exceeds the token limits Long-Term: Stores externally (e.g., vector DB) Can persist across sessions, days, and users Includes past decisions, tool calls, and user feedback. tldr; Short-Term memory is whatever fits inside the token window. Long-term memory is what you retrieve and then inject into that window. We'll build this in this session, using a lightweight in-memory DB ( faiss ) and a retrieval module we plug into our MCP agent loop. Vector DBs & embeddings Key Concepts: Embeddings: Vectors that represent meaning. Similarity Search: Find chunks most semantically similar to a query FAISS: Facebook AI Similarity Search - fast, memory-efficient index Embeddings are like a meaning ingerprint. You don't need exact words - you need closeness in idea space. Remember faiss_simple.py? \"Birds of a feather\" - matches \"People with similar traits stick together\". Let's look at a RAG implementation using MCP The zipped Download zipped code (with code and my index as well) Assignment Build a Chrome plugin that: For every web page that you visit (skip confidential ones", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_5", "timestamp": "2025-04-27T03:27:43.710Z"}, {"url": "https://canvas.instructure.com/courses/11514125/pages/session-7-memory?module_item_id=129973349", "title": "Session 7 - Memory: EAG V1", "chunk": "similar traits stick together\". Let's look at a RAG implementation using MCP The zipped Download zipped code (with code and my index as well) Assignment Build a Chrome plugin that: For every web page that you visit (skip confidential ones like Gmail, WhatsApp, etc), builds a nomic embedding (or any other model that you can run) and then builds an FAISS index with URL Please remember that you only need the index file, so you can do it on Google Colab and then download this index file as well When you search within your plugin, it opens the website where that content is and highlights it as well! 1000 Pts for above OR 2000 Pts total if you come up with an amazing idea to use RAG (locally or on browser). Share your YouTube Video and GitHub link. HOPING YOU REMEMBER TO ALWAYS HAVE THOSE 5 FILES (AGENT, PERCEPTION, MEMORY, ACTION AND DECISION) Video Studio (Transcript Download Transcript ) GMeet Previous Next", "chunk_id": "https___canvas.instructure.com_courses_11514125_pages_session-7-memory?module_item_id=129973349_6", "timestamp": "2025-04-27T03:27:43.710Z"}]